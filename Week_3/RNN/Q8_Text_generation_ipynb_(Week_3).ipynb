{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "# Text Generation with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpaIgZGRrSaw"
      },
      "source": [
        "* RNN을 사용하여 어떻게 텍스트를 생성하는지 알아보자.\n",
        "* 주어진 텍스트를 기반으로 텍스트를 생성하는 모델을 구현해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "### 생성의 한계\n",
        "\n",
        "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않다.\n",
        "\n",
        "이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n",
        "\n",
        "* 데이터는 문자 기반이다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르고 심지어 텍스트의 단위가 단어라는 것도 모른다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46TMSZVszoc9"
      },
      "source": [
        "### Drive 연결\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiWZ-S31zokw"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjWuSiewzw-R",
        "outputId": "3da4113e-3283-4d24-8bda-1ac75b1660f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### 셰익스피어 데이터셋 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### 데이터 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK",
        "outputId": "1ae67cc9-3b56-4b11-9622-1277af3a1692",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 데이터를 불러와서 디코딩\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 문자의 수\n",
        "print ('텍스트의 길이: {}'.format(len(text)))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텍스트의 길이: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO",
        "outputId": "da4a227b-76f7-4f29-900e-a6a3322b6a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 텍스트 처음 250자 출력\n",
        "print(text[1500:2500])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR",
        "outputId": "c1e597f1-bb98-4514-8e55-055aa34dcaa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 파일의 고유 문자수를 출력\n",
        "vocab = sorted(set(text)) # 내가 불러온 text 데이터를 집합(set)으로 만들어서 정렬시킨 상태입니다.\n",
        "# 알파벳 대,소문자 & 특수문자 처리\n",
        "print ('고유 문자수 {}개'.format(len(vocab)))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "고유 문자수 65개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 텍스트 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 텍스트 벡터화\n",
        "\n",
        "* 학습을 위해서 텍스트들을 수치화할 필요가 있다.\n",
        "* 텍스트를 인덱스화 시켜 학습에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# 고유 문자에서 인덱스로 매핑 생성\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab) # idx <=> char 변환할 수 있는 사전을 들고있는 것이 중요합니다.\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]) # 람다식"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPQ2A1mwOQUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cddf9a-7e34-4f7f-ca04-24a0c436fcc3"
      },
      "source": [
        "idx2char"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
              "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
              "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
              "      dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdVhhjYa31Uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5188e3-fc73-4d28-d111-803835034a4f"
      },
      "source": [
        "print(text_as_int)\n",
        "text_as_int.shape"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 ... 45  8  0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "* 텍스트 0번부터 전체 텍스트 길이까지 인덱스화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYyNlCNXymwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85c1959-43dd-42dd-950c-3a934e15b8d8"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(65)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  'H' :  20,\n",
            "  'I' :  21,\n",
            "  'J' :  22,\n",
            "  'K' :  23,\n",
            "  'L' :  24,\n",
            "  'M' :  25,\n",
            "  'N' :  26,\n",
            "  'O' :  27,\n",
            "  'P' :  28,\n",
            "  'Q' :  29,\n",
            "  'R' :  30,\n",
            "  'S' :  31,\n",
            "  'T' :  32,\n",
            "  'U' :  33,\n",
            "  'V' :  34,\n",
            "  'W' :  35,\n",
            "  'X' :  36,\n",
            "  'Y' :  37,\n",
            "  'Z' :  38,\n",
            "  'a' :  39,\n",
            "  'b' :  40,\n",
            "  'c' :  41,\n",
            "  'd' :  42,\n",
            "  'e' :  43,\n",
            "  'f' :  44,\n",
            "  'g' :  45,\n",
            "  'h' :  46,\n",
            "  'i' :  47,\n",
            "  'j' :  48,\n",
            "  'k' :  49,\n",
            "  'l' :  50,\n",
            "  'm' :  51,\n",
            "  'n' :  52,\n",
            "  'o' :  53,\n",
            "  'p' :  54,\n",
            "  'q' :  55,\n",
            "  'r' :  56,\n",
            "  's' :  57,\n",
            "  't' :  58,\n",
            "  'u' :  59,\n",
            "  'v' :  60,\n",
            "  'w' :  61,\n",
            "  'x' :  62,\n",
            "  'y' :  63,\n",
            "  'z' :  64,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VKcQHcymwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06de002b-ff6c-4858-be56-d36374384736"
      },
      "source": [
        "# 텍스트 맵핑\n",
        "print ('{} ---- Index ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen' ---- Index ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### 예측 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까?\n",
        "\n",
        "* 이는 모델을 훈련하여 수행할 작업이다.\n",
        "* 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측한다.\n",
        "* 이 출력은 현재 타임 스텝(time step)의 다음 문자이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 훈련 샘플과 타깃 만들기\n",
        "\n",
        "* 다음으로 텍스트를 샘플 시퀀스로 나누자.\n",
        "\n",
        "* 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함된다.\n",
        "\n",
        "* 각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함한다.\n",
        "\n",
        "* 텍스트를`seq_length + 1`개의 청크(chunk)로 나누자\n",
        "    * 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 된다.\n",
        "\n",
        "* 이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXwjMSQbNnD"
      },
      "source": [
        "# Hello 라는 단어를 만들고 싶습니다.\n",
        "# 학습 데이터를 아래와 같이 세팅을 해줘야합니다.\n",
        "# batch: 2번 사용.\n",
        "# input : H -> e -> l -> l\n",
        "# output : e -> l -> l -> o"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a6da2b-2758-4cce-e7b2-0e94585ab18f"
      },
      "source": [
        "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 훈련 샘플/타깃 만들기, 텍스트를 tf.data.Dataset로 변환, 텍스트를 벡터화 시킨걸 들고와서 각 알파벳과 숫자를 연결\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 알파벳을 하나씩 생성합니다.\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(i.numpy())\n",
        "    print(idx2char[i.numpy()])#확인가능"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "F\n",
            "47\n",
            "i\n",
            "56\n",
            "r\n",
            "57\n",
            "s\n",
            "58\n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "* `batch`를 이용해 몇개의 텍스트를 가져올 것인지 정할 수 있다.\n",
        "* drop_remainder=True: 길이가 길거나, 길이가 부족하면 버리는 옵션\n",
        "\n",
        "* 예를 들어, seq_length는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 된다.\n",
        "\n",
        "* seq_length + 1 크기의 시퀀스로 분할\n",
        "\n",
        "```\n",
        "input sequence = \"Hell\"\n",
        "output sequence = \"ello\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4hkDU3i7ozi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cc6e89-0084-4f28-afce-474af5a7078d"
      },
      "source": [
        "# seq_length + 1 크기의 시퀀스로 분할\n",
        "# 텍스트를 seq_length + 1개의 청크(chunk)로 나누자\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True) # 문장을 가져와합니다.\n",
        "                               # 문장을 가져올 수 있도록 batch 를 구성해줍니다.\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()]))) # repr 개행문자 출력"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 텍스트와 타깃 텍스트를 복사 및 이동\n",
        "* 모델이 Text를 생성할때, 한 텍스트 단위로 생성하기 때문에 그 다음 텍스트를 target으로 사용\n",
        "\n",
        "* tf.data.Dataset.from_tensor_slices 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환한다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "input_text = chunk[:-1] # chunk의 처음부터 마지막 문자 바로 앞까지의 부분을 선택\n",
        "target_text = chunk[1:] # chunk의 두 번째 문자부터 마지막 문자까지의 부분을 선택\n",
        "\n",
        "[:-1] 이면 -> 뒤부터 리스트를 본다. h(-5), e(-4), l(-3), l(-2), o(-1) 이순서.\n",
        "뒤순서로 -2부터 본다는 의미(-1)은 패스. -2~-5, hell만 본다.\n",
        "\n",
        "[1:] 이면 -> 앞부터 리스트를 본다. h(0), e(1), l(2), l(3), o(4) 이순서.\n",
        "앞에서 1번째 칸부터 본다는 말이므로 h는 패스, ello만 본다.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "# Hello -> 불러온 데이터의 길이\n",
        "# input : Hell # input의 길이\n",
        "# output : ello # output의 길이\n",
        "\n",
        " # 입력과 타깃 시퀀스 생성\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1] # chunk의 처음부터 마지막 문자 바로 앞까지의 부분을 선택\n",
        "    target_text = chunk[1:] # chunk의 두 번째 문자부터 마지막 문자까지의 부분을 선택\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "첫 번째 샘플의 타깃 값을 출력해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b1349e-3108-4b45-fc40-8965d49a0176"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 데이터:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "타깃 데이터:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "* 이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측한다.\n",
        "\n",
        "* 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트**(context)**를 고려한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBu9WZG84i0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d944664-7c26-4571-d50f-3fd94ff6959f"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"iter {:4d}\".format(i))\n",
        "    print(\"  inputs: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  generated text: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter    0\n",
            "  inputs: 18 ('F')\n",
            "  generated text: 47 ('i')\n",
            "iter    1\n",
            "  inputs: 47 ('i')\n",
            "  generated text: 56 ('r')\n",
            "iter    2\n",
            "  inputs: 56 ('r')\n",
            "  generated text: 57 ('s')\n",
            "iter    3\n",
            "  inputs: 57 ('s')\n",
            "  generated text: 58 ('t')\n",
            "iter    4\n",
            "  inputs: 58 ('t')\n",
            "  generated text: 1 (' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 훈련 배치 생성\n",
        "\n",
        "* 텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 `tf.data`를 사용\n",
        "* 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "# 배치 크기\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 알파벳을 기준으로 했던 데이터에서 문장단위로 데이터를 불러오는 dataset을 구성할 수 있습니다.\n",
        "for t, l in dataset:\n",
        "  print(t)\n",
        "  print(l)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPaUFqy5k5ai",
        "outputId": "430246bb-1eef-459d-9b16-5c80b3ebe15d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[47 52 45 ... 43 58 58]\n",
            " [50 42  1 ...  0 13 50]\n",
            " [43 58  1 ... 58 46 39]\n",
            " ...\n",
            " [12  0  0 ...  6  0 18]\n",
            " [39 54 54 ... 43 43 54]\n",
            " [53 46 43 ... 24 24 27]], shape=(32, 100), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[52 45  1 ... 58 58 50]\n",
            " [42  1 53 ... 13 50 40]\n",
            " [58  1 46 ... 46 39 58]\n",
            " ...\n",
            " [ 0  0 32 ...  0 18 53]\n",
            " [54 54 56 ... 43 54 43]\n",
            " [46 43 51 ... 24 27 10]], shape=(32, 100), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "모델을 정의하려면 `tf.keras.Sequential`을 사용한다.\n",
        "\n",
        "이 예제에서는 3개의 층을 사용하여 모델을 정의한다:\n",
        "\n",
        "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
        "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있다.)\n",
        "* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층.\n",
        "\n",
        "각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 타임 스텝으로 실행하고, FC layers를 적용하여 다음 문자의 로그 가능도(log-likelihood)를 예측하는 로짓을 생성한다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\n",
        "vocab_size = len(vocab) # 모델이 처리할 수 있는 고유 문자의 총 수\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256 # 각 문자를 표현하는 임베딩 벡터의 차원\n",
        "\n",
        "# RNN 유닛(unit) 개수\n",
        "rnn_units = 1024 # RNN 레이어에서 사용되는 유닛(뉴런)의 수, 정해진건 없음. but LSTM 경우 layer가 적어서 많이 값을 써줘야함"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        # 임베딩 층: 각 문자를 embedding_dim 차원의 벡터로 변환\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "        # LSTM 층: 시퀀스 데이터 처리\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True, # 각 시퀀스의 출력을 반환 (이 입력 시퀀스가 들어올때 값을 다음으로 예측해야 하서 True)\n",
        "                            stateful=True, # 배치 간 상태 유지\n",
        "                            recurrent_initializer='glorot_uniform'), # 초기화 방식\n",
        "        # Dense 층: 출력을 vocab_size의 크기로 변환\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgzQLegT2H_J"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbNnqjvhI9lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e9d842-80ed-41aa-d395-f12b1477dc88"
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.output_shape)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, None, 256)\n",
            "(32, None, 1024)\n",
            "(32, None, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## 모델 사용\n",
        "\n",
        "이제 모델을 실행하여 원하는대로 동작하는지 확인해보자.\n",
        "\n",
        "먼저 출력의 형태를 확인하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff421dad-6310-4ab1-c245-8c9519ed6f1e"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 65) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "위 예제에서 입력의 시퀀스 길이는 100이지만 모델은 임의 시퀀스 길이의 입력도 사용 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f244e200-bc55-4003-b61a-b2cc2a2dba05"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (32, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (32, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_6 (Dense)             (32, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "이 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Optimizer, Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "`tf.keras.losses.sparse_softmax_crossentropy` 를 사용해 label을 벡터로 바꾸지 않고 loss를 계산한다.\n",
        "\n",
        "이 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf6a455-56ba-47be-dd00-38d93746e4ab"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")\n",
        "print(\"Loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 배열 크기(shape):  (32, 100, 65)  # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n",
            "Loss:  4.1746273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "### 학습준비\n",
        "* `tf.keras.Model.compile` 메서드를 사용하여 훈련 절차를 설정\n",
        "* 기본 매개변수의 `tf.keras.optimizers.Adam`과 손실 함수를 사용\n",
        "* categorical_crossentropy식이 아니라, 다른 가중치 or 연산 추가 해서 custom loss\n",
        "* custom loss가 추가되는 이유: 연산을 할때 모델에 맞게 잘 구현하기 위해서 사용 (모델의 학습 방식 변경)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=loss)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 체크포인트 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# the save point\n",
        "if use_colab:\n",
        "    checkpoint_dir ='./drive/My Drive/train_ckpt/text_gen/exp1'\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "else:\n",
        "    checkpoint_dir = 'text_gen/exp1'\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='loss',\n",
        "                                                 mode='auto',\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### 훈련 실행\n",
        "* 생성모델은 loss가 크게 나올수도 있음 -> 그렇다고 결과가 안좋다는건 아님\n",
        "* loss 자체가 얼마나 떨어졌는지를 보는 경우도 있다.\n",
        "* loss 거이 안떨어졌거나 상승시, 잘못 만들었구나 라고 판단.\n",
        "* 생성 모델은 지표보단, 만든 결과물 보고 사람이 판단 하는걸로 판단"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS=5"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee5b8b8-5b4a-417a-d330-9e16f2c2fa67"
      },
      "source": [
        "# 생성 모델은 loss와 상관없는 경우도 있음\n",
        "history = model.fit(dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "344/345 [============================>.] - ETA: 0s - loss: 2.3357\n",
            "Epoch 1: loss improved from inf to 2.33428, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "345/345 [==============================] - 19s 47ms/step - loss: 2.3343\n",
            "Epoch 2/5\n",
            "345/345 [==============================] - ETA: 0s - loss: 1.6760\n",
            "Epoch 2: loss improved from 2.33428 to 1.67600, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "345/345 [==============================] - 18s 44ms/step - loss: 1.6760\n",
            "Epoch 3/5\n",
            "345/345 [==============================] - ETA: 0s - loss: 1.4930\n",
            "Epoch 3: loss improved from 1.67600 to 1.49297, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "345/345 [==============================] - 17s 43ms/step - loss: 1.4930\n",
            "Epoch 4/5\n",
            "344/345 [============================>.] - ETA: 0s - loss: 1.4024\n",
            "Epoch 4: loss improved from 1.49297 to 1.40233, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "345/345 [==============================] - 17s 43ms/step - loss: 1.4023\n",
            "Epoch 5/5\n",
            "344/345 [============================>.] - ETA: 0s - loss: 1.3443\n",
            "Epoch 5: loss improved from 1.40233 to 1.34431, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "345/345 [==============================] - 17s 44ms/step - loss: 1.3443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 최근 체크포인트 복원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "이 예측 단계에선 Batch size 1을 사용한다.\n",
        "\n",
        "* RNN 상태가 타임 스텝에서 타임 스텝으로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용\n",
        "* 다른 배치 크기로 모델을 실행하려면 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycQ-ot_jjyu"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(checkpoint_dir)\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델은 각각 별개로 봐야함\n",
        "* 모델을 학습 하고 checkpoint로 저장하고, 아래 layer에서 inference code 를 가져와서 진행함\n",
        "* 아래에서 위에 만든 뼈대를 가져오고 저장후 다시 아래에서 위의걸 가져오고 그런 방식"
      ],
      "metadata": {
        "id": "8h5vFnBJajpY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xa6jnYVrAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0e9fc6-a95b-4701-8620-7fbb0c3aa21e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 예측 루프\n",
        "\n",
        "텍스트 생성:\n",
        "\n",
        "* input들이 누적되게 하도록 하는 방식으로\n",
        "\n",
        "* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정\n",
        "\n",
        "* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자의 예측 배열을 가져온다.\n",
        "\n",
        "* 다음, 범주형 배열을 사용하여 예측된 문자의 인덱스를 계산\n",
        "\n",
        "* 이 예측된 문자를 모델의 다음 입력으로 활용\n",
        "\n",
        "* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 하나의 단어가 아닌 더 많은 컨텍스트를 갖추게 된다.\n",
        "\n",
        "* 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식\n",
        "\n",
        "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "* 생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
        "\n",
        "  # 생성할 문자의 수\n",
        "  num_generate = 1000 # 작은 모델 (숫자-token) 1000번을 넘어갈때 까지는 멈추지 않음\n",
        "\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 결과를 저장할 빈 문자열\n",
        "  text_generated = []\n",
        "\n",
        "  # 온도가 낮으면 더 예측 가능한 텍스트 생성\n",
        "  # 온도가 높으면 더 의외의 텍스트 생성 (불확실성)\n",
        "  # 최적의 세팅을 찾기 위한 실험\n",
        "  # 생성된 텍스트의 예측 가능성과 다양성을 조절하는 데 사용됩니다.\n",
        "  # 낮은 온도는 더 일관되고 예측 가능한 텍스트를,\n",
        "  # 높은 온도는 더 다양하고 예상치 못한 텍스트를 생성\n",
        "  temperature = 1.0 # 텍스트의 예측 확률 조작 (생성 결과물 변경 가능)\n",
        "\n",
        "  # 여기에서 배치 크기 == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 배치 차원 제거\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
        "      predictions = predictions / temperature\n",
        "       # vocab(65)에서 랜덤하게 추출해서 생성\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
        "      # 이전 은닉 상태와 함께\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktovv0RFhrkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce9ad6e-7aff-420d-e6b4-d0bcaaace0e8"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: &ZQQxXX&FX&QVXSXxXXXjQ3$QJXXQJx$X&X&$QQ$XXJX$JQ&X$Q&ZQ&XX3QXXzQxX&X&VpXQXzXQJMQX3J$QNXQEzX&QXzXXX&JX&QQjxz&XNXQX&&&X$JXXX3$&;$S3Qxq3X-$XXQX$XXMXQ$XXXXxQXXXJJXXQXXZXXQRXzJX&QXzXXQ&QXzX&&QXXXQJX$&XJQxX&XXZXQZ&XQ&QKXXXDjX$X&&MJXQKUJQQYQQQzXxX&XXXXzQXZXXXQ$&JXXJ3QX;$&xxXQXjXXxqXQXQXXXU&Z&MX3QZQJJU3JQ$xQXV&QQJNX$zXXjxXZ&X3X3DJZXXvIXxqXQ3qXQ&pY&vXx3Z&QUKQQXJXZXZQXzXX&XD&x$QXQQXZ&xUXQ&QQQQXXZzHFVXXX3JX&XxxFSxXXjXX&z&XQ$ZKxZzQ&Jq!X&XjXJ&XQXXXQ&XQZXjXQXQQXQJYX&XXZQXQ$Qx&XxXXQXQ&XXXXZXZxXQqXXX3zXXXVxXX3JzQJQ3vXcjXqxYXXq&Q&&XXUQEJXxQZXjXjzQU&ZXXJQQUQQXxX&Dz&QJ&jxxXQXZXXJQJXXQxX&UQXXXXQJ&XQQQjF$XKX.jJXXQXx&$xX&&&X&XMXXQXXQ$qQ$Q$xQXxzXQXxx&U&XX&SXMxSxxx&QMqJZXXXXQ$YxXQ3&QXxVXKUQQJXQ&X$K3U&x$XKXXQx&x&&Q$QQQUXXX&ZXJQ&$XQQIQXXQJX&XQ&X&XXX&XjX&XQXQQS?QQXVNX3zX&QQ&X&&X&XX&YXqXJQSXQJQXXQXZXX&XXX&ZXXD3XQKJZx$zQXXJXjzQXxX$&XJQ&XXAQX&XJxx&XXzX&$X&xXXQNXJzJX&X&x&xExJXXX&Q$XXzXQ&XX&XXQ&X3QXXKDx$XXQ&XNNXxX3XE&KXxX&j3Z&&&&XQqXXQZQZjXQKU&3QXXqXDx$zxx$XXXQXX&Q&XpQXJQXQKUXXXQXXQXQXzxXXXJ$XXXZXJXXXQUXX$XxDQjXX$XXXX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5NJ0DrPzJsz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}