{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wZmjKwlBSVnx"
      ],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu1-_CuosHDh"
      },
      "source": [
        "# Speech spectrogram classification\n",
        "\n",
        "## Spectrogram 데이터를 이용해 음성을 분류하는 모델을 제작해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TKZ-SAg2AIJ"
      },
      "source": [
        "## challenge\n",
        "### Task\n",
        "* 1초 길이의 오디오 음성데이터를 이용해 단어를 분류하는 것이 목표입니다.\n",
        "* 주어진 데이터를 이용해 딥러닝 트레이닝 과정을 구현해 보는것이 목표입니다.\n",
        "* This code is borrowed from [Kaggle/TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge).\n",
        "* This is version 0.01 of the data set containing 64,727 audio files, released on August 3rd 2017.\n",
        "* **챌린지에서 사용하는 데이터는 Wave에서 Spectrogram으로 변환된 데이터입니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvN8sTwi4S0t"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "* 우리가 사용할 packages 를 import 하는 부분 입니다.\n",
        "* 필요에 따른 packages를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlMAFGSosHDk"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from os.path import isdir, join\n",
        "\n",
        "import random\n",
        "import copy\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XgiqM0DsHDo"
      },
      "source": [
        "### Setting Dataset\n",
        "\n",
        "* Colab 적용을 위한 변수 지정 및 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU1jUvnosMRH"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSd1jUmOym_t"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXI0z5vEsHD9"
      },
      "source": [
        "if use_colab:\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/Datasets\"\n",
        "else:\n",
        "    DATASET_PATH = \"./\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeGRTrMHZaRL"
      },
      "source": [
        "### Dataset Shape\n",
        "* 불러온 데이터셋의 shape을 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLIdaMhCyUFZ"
      },
      "source": [
        "data_wav = np.load(os.path.join(DATASET_PATH, \"speech_spec_8000.npy\"))\n",
        "print(data_wav.shape)\n",
        "# 50620, 130, 126, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(-1, data_wav.shape[1], data_wav.shape[2], 1)"
      ],
      "metadata": {
        "id": "AGk9vyKxNQlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auWK1iE3-eYh"
      },
      "source": [
        "* Spectrogram으로 변환한 데이터를 plot 해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnIV5RMEsHEG"
      },
      "source": [
        "librosa.display.specshow(librosa.amplitude_to_db(data_wav[219], ref=np.max), x_axis='time')\n",
        "plt.title('Power spectrogram')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7_lX4sP-l0r"
      },
      "source": [
        "* 전체 데이터셋의 wave 데이터를 spectrogram으로 변환한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0w98NJJehoq"
      },
      "source": [
        "### Target_list 설정\n",
        "* 데이터셋은 기본적으로 총 12개의 클래스로 나누어져있다.\n",
        "```\n",
        "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
        "```\n",
        "* 해당 클래스로 나누어진 label을 학습 가능한 형태로 처리 후 데이터셋 제작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu1VDu_p9Nq9"
      },
      "source": [
        "data_label = np.load(os.path.join(DATASET_PATH, \"speech_label_8000.npy\"))\n",
        "print(data_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RDioDCsHEX"
      },
      "source": [
        "# label 전처리\n",
        "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
        "##################################################\n",
        "##### 주어진 label => idx 형태로 변경해주셔야합니다. #####\n",
        "##################################################\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bwwnfZisHES"
      },
      "source": [
        "### Model dataset setting\n",
        "* 변환된 데이터를 이용해서 학습에 활용할 데이터셋을 설정한다.\n",
        "    * data -> data_wav\n",
        "    * label -> data_label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FUgL7TSsHEU"
      },
      "source": [
        "train_wav, test_wav, train_label, test_label = train_test_split(#)\n",
        "\n",
        "# reshape for conv layers Conv2D -> 차원이 더 늘어납니다. 데이터 shape도 바뀝니다!\n",
        "train_wav = train_wav.reshape(# TODO)\n",
        "test_wav = test_wav.reshape(# TODO)\n",
        "\n",
        "print(train_wav.shape)\n",
        "print(test_wav.shape)\n",
        "print(train_label.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "del data_wav # 메모리 관리를 위해 변수 삭제\n",
        "del data_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ9yGwb6sHEe"
      },
      "source": [
        "print('Train_Wav Demension : ' + str(np.shape(train_wav)))\n",
        "print('Train_Label Demension : ' + str(np.shape(train_label)))\n",
        "print('Test_Wav Demension : ' + str(np.shape(test_wav)))\n",
        "print('Test_Label Demension : ' + str(np.shape(test_label)))\n",
        "print('Number Of Labels : ' + str(len(label_value)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvck1awy3Md"
      },
      "source": [
        "### Hyper-parameters setting\n",
        "* 학습 전반에서 사용할 batch size, epoch, checkpoint dir을 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt-aLitgy5xI"
      },
      "source": [
        "# the save point\n",
        "if use_colab:\n",
        "    checkpoint_dir ='./drive/MyDrive/train_ckpt/spectrogram/exp1'\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "else:\n",
        "    checkpoint_dir = 'spectrogram/exp1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JnCHOjS7H2q"
      },
      "source": [
        "### Dataset 구성\n",
        "* 전처리가 완료된 데이터들을 이용해서 Train, Test Dataset을 직접 구성해봅시다.\n",
        "* 학습에 사용할 Loss Function의 설정을 고려해 제작\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65xPCvazsHEm"
      },
      "source": [
        "# 전체 데이터셋 구성\n",
        "batch_size = # TODO\n",
        "\n",
        "# for train\n",
        "train_dataset = # TODO\n",
        "print(train_dataset)\n",
        "\n",
        "# for test\n",
        "test_dataset = # TODO\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZmjKwlBSVnx"
      },
      "source": [
        "### Dataset 구성 검증\n",
        "```\n",
        "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
        "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykQEO8XP7FFD"
      },
      "source": [
        "## Model 구현\n",
        "* 제시된 모델을 구현해보고, 더 좋은 성능으로 튜닝해보자.\n",
        "\n",
        "    * inputs = [batch_size, 130, 126, 1]\n",
        "    * conv1 = [batch_size, 65, 63, 16]\n",
        "    * conv2 = [batch_size, 33, 32, 32]\n",
        "    * conv3 = [batch_size, 17, 16, 64]\n",
        "    * desne = [batch_size, 64]\n",
        "    * output = [batch_size, 12]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MhnpbAHsHEo"
      },
      "source": [
        "input_tensor = # TODO\n",
        "\n",
        "x = # TODO\n",
        "print(x.shape)\n",
        "\n",
        "output_tensor = # TODO\n",
        "\n",
        "model = tf.keras.Model(# TODO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeBqtI2tkAIo"
      },
      "source": [
        "* 구현된 모델을 어떻게 학습시킬 것인지 구성해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90AizrGdsHEr"
      },
      "source": [
        "model.compile(# TODO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32hdTHj7SeCN"
      },
      "source": [
        "### 모델 Output 확인\n",
        "* 총 12개의 예측 데이터가 출력되는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg5pReaesHEt"
      },
      "source": [
        "# without training, just inference a model:\n",
        "predictions = model(train_wav[0:1], training=False)\n",
        "print(\"Predictions: \", predictions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDegoCoskGef"
      },
      "source": [
        "* 최종 모델 구성을 확인 후 모델을 저장할 체크포인트를 구성해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfQfGlfNsHEy"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WEIV2-yzejQ"
      },
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='val_loss',\n",
        "                                                 mode='auto', # max, min, auto\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)\n",
        "# 추가 callback 함수를 구현해보세요!\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIr8LoVs6zdb"
      },
      "source": [
        "## Training\n",
        "* 위에서 구현한 데이터셋, 모델들을 fit 함수를 이용해 학습을 시켜봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_KCHDbsHE2"
      },
      "source": [
        "# model.fit model.fit_generator는 model.fit으로 통일되었습니다.\n",
        "# tf.data.Dataset은 generator 입니다.\n",
        "history = model.fit(# TODO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMk4bWh1kquU"
      },
      "source": [
        "* 학습 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep7mink9sHE4"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRjMdOed6B9U"
      },
      "source": [
        "## Evaluation\n",
        "* Test dataset을 이용해서 모델의 성능을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtNWcBnq0LXs"
      },
      "source": [
        "model.load_weights(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjaqdVwjsHE7"
      },
      "source": [
        "results = model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3oIE73DSxFR"
      },
      "source": [
        "### 스코어 결과\n",
        "* 위의 스코어는 분류모델에 적용되는 스코어입니다.\n",
        "* 모델의 크기 (MB) 와 정확도를 이용해 스코어를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srOVztfNsHE_"
      },
      "source": [
        "def final_score():\n",
        "    print(\"Model params num : \" + str(model.count_params()))\n",
        "    print(\"Accuracy : \" + str(results[1]))\n",
        "\n",
        "    s = (model.count_params() * 32) / (1024 ** 2)\n",
        "    score = 50 * (results[1] + min((1/s), 1))\n",
        "\n",
        "    print(\"score : \" + str(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akh6IYvsSyg3"
      },
      "source": [
        "final_score()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}