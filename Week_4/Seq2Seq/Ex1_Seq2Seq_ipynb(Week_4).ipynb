{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suKRzjIa2P0S"
      },
      "source": [
        "# Sequence to Sequence\n",
        "\n",
        "### simple neural machine translation training\n",
        "\n",
        "* sequence to sequence\n",
        "  \n",
        "### Reference\n",
        "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vhwgxAJL1TW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import font_manager, rc\n",
        "\n",
        "# rc('font', family='AppleGothic') #for mac\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5H57Jdu2P0U"
      },
      "outputs": [],
      "source": [
        "sources = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "targets = [['나는', '배가', '고프다'],\n",
        "           ['텐서플로우는', '매우', '어렵다'],\n",
        "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
        "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_MeI8302P0U"
      },
      "outputs": [],
      "source": [
        "# vocabulary for sources\n",
        "# 얻은 단어들을 이용하여 어휘집을 만들고, 이를 정렬한 뒤에 각 단어에 고유한 인덱스를 할당하는 과정을 수행\n",
        "s_vocab = list(set(sum(sources, [])))\n",
        "s_vocab.sort()\n",
        "s_vocab = ['<pad>'] + s_vocab\n",
        "source2idx = {word : idx for idx, word in enumerate(s_vocab)} # 각 단어에 대해 고유한 인덱스를 할당 -> 사전\n",
        "idx2source = {idx : word for idx, word in enumerate(s_vocab)} # 인덱스를 키로 하고 단어를 값으로 하는 사전\n",
        "\n",
        "pprint(source2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYEraIYt2P0V"
      },
      "outputs": [],
      "source": [
        "# vocabulary for targets\n",
        "# 깃 데이터에서 사용되는 단어들로 어휘집을 만들고, 이에 인덱스를 할당\n",
        "\n",
        "t_vocab = list(set(sum(targets, []))) # targets라는 리스트의 리스트에서 모든 단어들을 추출 & 중복 제거 -> 리스트화: t_vocab\n",
        "t_vocab.sort() # 알파벳 순으로 정렬\n",
        "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab # 특수 토큰 추가, '<pad>'는 패딩을 위한 토큰, '<bos>'와 '<eos>'는 각각 문장의 시작과 끝을 나타내는 토큰\n",
        "target2idx = {word : idx for idx, word in enumerate(t_vocab)} # 각 단어에 대해 고유한 인덱스를 할당하는 사전\n",
        "idx2target = {idx : word for idx, word in enumerate(t_vocab)} # 인덱스를 키로 하고 단어를 값으로 하는 사전\n",
        "\n",
        "pprint(target2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sequence 데이터 전처리 (source, target 2가지의 작업 모드 지원)\n",
        "\n",
        "### source 모드 (인코더를 위한 전처리)\n",
        "1. 시퀀스 토큰화: 입력된 sequences의 각 문장을 토큰화하고, 이 토큰들을 dic 사전을 사용하여 해당 인덱스로 변환합니다.\n",
        "2. 문장 길이 계산: 변환된 각 문장의 길이를 계산합니다.\n",
        "3. 패딩 및 자르기: pad_sequences 함수를 사용하여 모든 문장을 max_len 길이로 맞춥니다. 길이가 max_len보다 짧은 문장은 뒤쪽에 패딩(<pad>)을 추가하고, 길이가 더 긴 문장은 뒷부분을 잘라냅니다.\n",
        "4. 결과 반환: 각 문장의 길이와 전처리된 입력 데이터를 반환합니다.\n",
        "\n",
        "### target 모드 (디코더를 위한 전처리)\n",
        "\n",
        "1. 시퀀스 토큰화 및 특수 토큰 추가: 입력된 sequences의 각 문장 앞에 <bos>를 추가하고, 끝에 <eos>를 추가한 뒤 토큰화합니다. 그런 다음 dic 사전을 사용하여 이 토큰들을 해당 인덱스로 변환합니다.\n",
        "2. 입력 문장 길이 계산: 변환된 각 입력 문장의 길이를 계산합니다.\n",
        "3. 입력 데이터 패딩 및 자르기: 입력 데이터에 대해 pad_sequences 함수를 사용하여 max_len 길이로 맞춥니다.\n",
        "4. 출력 데이터 준비: 원래 sequences의 각 문장 끝에 <eos>를 추가한 후 토큰화하고, dic 사전을 사용하여 인덱스로 변환합니다. 그 후 pad_sequences를 사용하여 max_len 길이로 맞춥니다.\n",
        "5. 결과 반환: 각 입력 문장의 길이, 전처리된 입력 데이터, 전처리된 출력 데이터를 반환합니다."
      ],
      "metadata": {
        "id": "MyMyzEzJ2dmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzeq7Dqi2P0V"
      },
      "outputs": [],
      "source": [
        "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
        "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
        "\n",
        "    if mode == 'source':\n",
        "        # preprocessing for source (encoder)\n",
        "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
        "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
        "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "        return s_len, s_input\n",
        "\n",
        "    elif mode == 'target':\n",
        "        # preprocessing for target (decoder)\n",
        "        # input\n",
        "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
        "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
        "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
        "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "\n",
        "        # output\n",
        "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
        "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
        "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
        "\n",
        "        return t_len, t_input, t_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktFkuKkf2P0V"
      },
      "outputs": [],
      "source": [
        "# preprocessing for source\n",
        "s_max_len = 10\n",
        "s_len, s_input = preprocess(sequences = sources,\n",
        "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
        "print(s_len, s_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dehocpn_2P0V"
      },
      "outputs": [],
      "source": [
        "# preprocessing for target\n",
        "t_max_len = 12\n",
        "t_len, t_input, t_output = preprocess(sequences = targets,\n",
        "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
        "print(t_len, t_input, t_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUSmMaSP2P0V"
      },
      "source": [
        "# hyper-param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuiFGFo12P0V"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "epochs = 200\n",
        "batch_size = 4\n",
        "learning_rate = .005\n",
        "total_step = epochs / batch_size\n",
        "buffer_size = 100\n",
        "n_batch = buffer_size//batch_size\n",
        "embedding_dim = 32\n",
        "units = 32\n",
        "\n",
        "# input\n",
        "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output)) #i/o 를 sequence 데이터 에서 받아옴.\n",
        "data = data.shuffle(buffer_size = buffer_size)\n",
        "data = data.batch(batch_size = batch_size)\n",
        "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVePi5RF2P0V"
      },
      "outputs": [],
      "source": [
        "def gru(units):\n",
        "    return tf.keras.layers.GRU(units,\n",
        "                               return_sequences=True,\n",
        "                               return_state=True,\n",
        "                               recurrent_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder - Seq2Seq Model\n",
        "- vocab_size: 어휘집의 크기. 즉, 모델이 처리할 수 있는 고유 단어의 수.\n",
        "- embedding_dim: 임베딩 차원. 각 단어를 나타내는 벡터의 크기.\n",
        "- enc_units: 인코더의 유닛(또는 뉴런) 수. 이는 GRU 레이어의 복잡성을 결정합니다.\n",
        "- batch_sz: 배치 크기. 모델이 한 번에 처리하는 데이터의 수.\n",
        "이 함수는 임베딩 레이어와 GRU 레이어를 초기화합니다."
      ],
      "metadata": {
        "id": "zjAWpn1T45-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs3TclgQ2P0W"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        # 입력된 단어 인덱스를 해당 임베딩 벡터로 변환 -> 단어의 의미를 수치적으로 표현하는 과정\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU (Gated Recurrent Unit) 레이어: 순차적인 데이터를 처리하기 위한 레이어 -> LSTM과 유사, 더 간단한 구조\n",
        "        self.gru = gru(self.enc_units)\n",
        "\n",
        "    # 임베딩 레이어를 통해 입력 데이터를 임베딩 벡터로 변환하고, GRU 레이어를 통해 출력(output)과 새로운 숨겨진 상태(state)를 생성\n",
        "    # 모델이 입력 데이터 x와 숨겨진 상태 hidden을 받아 처리하는 함수\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "\n",
        "        return output, state\n",
        "    # 인코더의 초기 숨겨진 상태를 생성. 이는 모든 요소가 0인 텐서로, 배치 크기와 인코더 유닛 수에 따라 결정\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder - Seq2Seq Model\n",
        "- vocab_size: 어휘집의 크기. 즉, 모델이 처리할 수 있는 고유 단어의 수.\n",
        "- embedding_dim: 임베딩 차원. 각 단어를 나타내는 벡터의 크기.\n",
        "- enc_units: 인코더의 유닛(또는 뉴런) 수. 이는 GRU 레이어의 복잡성을 결정합니다.\n",
        "- batch_sz: 배치 크기. 모델이 한 번에 처리하는 데이터의 수. 이 함수는 임베딩 레이어와 GRU 레이어를 초기화합니다."
      ],
      "metadata": {
        "id": "vZFvJ-JC588I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5oy9_Vw2P0W"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        # 입력된 단어 인덱스를 해당 임베딩 벡터로 변환 -> 단어의 의미를 수치적으로 표현하는 과정\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU (Gated Recurrent Unit) 레이어: 순차적인 데이터를 처리하기 위한 레이어 -> LSTM과 유사, 더 간단한 구조\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # 임베딩 레이어를 통해 입력 데이터를 임베딩 벡터로 변환하고, GRU 레이어를 통해 출력(output)과 새로운 숨겨진 상태(state)를 생성\n",
        "    # 모델이 입력 데이터 x와 숨겨진 상태 hidden을 받아 처리하는 함수\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state\n",
        "\n",
        "    # 인코더의 초기 숨겨진 상태를 생성. 이는 모든 요소가 0인 텐서로, 배치 크기와 인코더 유닛 수에 따라 결정\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QETtRBH-2P0W"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
        "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "\n",
        "#     print(\"real: {}\".format(real))\n",
        "#     print(\"pred: {}\".format(pred))\n",
        "#     print(\"mask: {}\".format(mask))\n",
        "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# creating optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# creating check point (Object-based saving)\n",
        "checkpoint_dir = './data_out/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder)\n",
        "\n",
        "# create writer for tensorboard\n",
        "#summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "- 어려운이유, 모델 2개가 합쳐저셔 학습이 복잡한편\n",
        "- incoder가 아무리 학습 & 압축을 잘한다고 해도 context에서 문제(데이터 손상)가 생기면 decoder 에서 (데이터가 잘 안받아지는)문제가 생길수 있음"
      ],
      "metadata": {
        "id": "QZhHvfqU8BAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4EzTNqW2P0W"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    # 매 에폭마다 인코더의 숨겨진 상태를 초기화합니다.\n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    # 에폭별 총 손실을 기록하기 위한 변수입니다.\n",
        "    total_loss = 0\n",
        "\n",
        "    # 데이터셋을 배치 단위로 순회하면서 각 배치에 대해 학습을 수행합니다.\n",
        "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
        "        # 각 배치에 대한 손실을 초기화합니다.\n",
        "        loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 인코더를 실행하여 출력과 숨겨진 상태를 얻습니다.\n",
        "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
        "\n",
        "            # 디코더의 초기 숨겨진 상태를 설정합니다.\n",
        "            dec_hidden = enc_hidden\n",
        "\n",
        "            # 디코더의 초기 입력을 설정합니다. 이는 각 시퀀스의 시작을 나타내는 <bos> 토큰입니다.\n",
        "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
        "\n",
        "            # Teacher Forcing 기법을 사용합니다. 이는 다음 입력으로 실제 타깃을 사용하는 방식입니다.\n",
        "            for t in range(1, t_input.shape[1]):\n",
        "                # 디코더를 실행하여 예측값과 새로운 숨겨진 상태를 얻습니다.\n",
        "                predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "                # 손실 함수를 사용하여 현재 시점의 손실을 계산합니다.\n",
        "                loss += loss_function(t_input[:, t], predictions)\n",
        "\n",
        "                # 다음 시점의 입력을 설정합니다. 이는 현재 타깃 시퀀스의 현재 토큰입니다.\n",
        "                dec_input = tf.expand_dims(t_input[:, t], 1)\n",
        "\n",
        "        # 배치에 대한 평균 손실을 계산합니다.\n",
        "        batch_loss = (loss / int(t_input.shape[1]))\n",
        "\n",
        "        # 에폭별 총 손실에 배치 손실을 추가합니다.\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        # 인코더와 디코더의 변수를 결합합니다.\n",
        "        variables = encoder.variables + decoder.variables\n",
        "\n",
        "        # 손실에 대한 변수들의 그레이디언트를 계산합니다.\n",
        "        gradient = tape.gradient(loss, variables)\n",
        "\n",
        "        # 계산된 그레이디언트를 사용하여 변수들을 업데이트합니다.\n",
        "        optimizer.apply_gradients(zip(gradient, variables))\n",
        "\n",
        "    # 일정 에폭마다 모델의 진행 상황을 출력하고 모델을 저장합니다.\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
        "                                            total_loss / n_batch,\n",
        "                                            batch_loss.numpy()))\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBLbR1jD2P0W"
      },
      "outputs": [],
      "source": [
        "#restore checkpoint\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PDr6gX_2P0W"
      },
      "outputs": [],
      "source": [
        "sentence = 'I feel hungry'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RdE9R6w-2P0W"
      },
      "outputs": [],
      "source": [
        "def prediction(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "\n",
        "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += idx2target[predicted_id] + ' '\n",
        "\n",
        "        if idx2target.get(predicted_id) == '<eos>':\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence\n",
        "\n",
        "result, output_sentence = prediction(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)\n",
        "\n",
        "print(sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tipfQgUx2P0W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}