{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jG82MsD9YPEn"},"outputs":[],"source":["# Transformers installation\n","! pip install transformers datasets\n","# To install from source instead of the last release, comment the command above and uncomment the following one.\n","# ! pip install git+https://github.com/huggingface/transformers.git"]},{"cell_type":"markdown","metadata":{"id":"bWIi6sDrYPEo"},"source":["# Fine-tune a pretrained model"]},{"cell_type":"markdown","metadata":{"id":"pt768tyUYPEp"},"source":["사전 학습된 모델을 사용하면 상당한 이점이 있습니다. 계산 비용과 탄소 배출량을 줄이고 처음부터 훈련하지 않고도 최신 모델을 사용할 수 있습니다. 🤗 Transformers는 다양한 작업을 위해 수천 개의 사전 훈련된 모델에 대한 액세스를 제공합니다. 사전 훈련된 모델을 사용하는 경우 작업에 특정한 데이터 세트에서 모델을 훈련합니다. 이 튜토리얼에서는 선택한 딥러닝 프레임워크로 사전 학습된 모델을 미세 조정합니다.\n","\n","* Fine-tune a pretrained model with 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer).\n","* Fine-tune a pretrained model in TensorFlow with Keras.\n","\n","<a id='data-processing'></a>"]},{"cell_type":"markdown","metadata":{"id":"6ad9QLLWYPEq"},"source":["## Prepare a dataset"]},{"cell_type":"markdown","metadata":{"id":"DzFuIrndYPEq"},"source":["사전 훈련된 모델을 미세 조정하기 전에 데이터 세트를 다운로드하고 훈련을 위해 준비하세요. \n","\n","Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnOWzpWgYPEq"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"yelp_review_full\")\n","dataset[\"train\"][100]"]},{"cell_type":"markdown","metadata":{"id":"Q98PAZXeYPEr"},"source":["텍스트를 처리하고 가변 시퀀스 길이를 처리하기 위한 패딩 및 truncation 전략을 포함하려면 토크나이저가 필요합니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcQAADoWYPEr"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"hUSU1cTkYPEr"},"source":["원하는 경우 전체 데이터 세트의 더 작은 하위 집합을 만들어 미세 조정하여 소요 시간을 줄일 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFprVB42YPEr"},"outputs":[],"source":["small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n","small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"]},{"cell_type":"markdown","metadata":{"id":"cMZ9jGOBYPEs"},"source":["<a id='trainer'></a>"]},{"cell_type":"markdown","metadata":{"id":"SralBKDRYPEs"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"Q0s0gHSwYPEs"},"source":["🤗 Transformers는 교육에 최적화된 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 클래스를 제공합니다. 자신의 훈련 루프. [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API는 로깅, 기울기 누적, 혼합 정밀도와 같은 다양한 훈련 옵션과 기능을 지원합니다.\n","\n","모델을 로드하여 시작하고 예상 레이블 수를 지정합니다. Yelp Review [데이터 세트 카드](https://huggingface.co/datasets/yelp_review_full#data-fields)에서 5개의 레이블이 있음을 알 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyFEy_lQYPEs"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"]},{"cell_type":"markdown","metadata":{"id":"YQwEc4UJYPEs"},"source":["### Training hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"cF-akgyNYPEs"},"source":["다음으로 조정할 수 있는 모든 하이퍼파라미터와 다양한 훈련 옵션을 활성화하기 위한 플래그가 포함된 [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) 클래스를 만듭니다. \n","\n","이 튜토리얼의 경우 기본 [하이퍼파라미터](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)로 시작할 수 있지만 자유롭게 실험하여 최적의 설정을 찾으십시오.\n","\n","교육에서 체크포인트를 저장할 위치를 지정합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHE2m__mYPEs"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\")"]},{"cell_type":"markdown","metadata":{"id":"kBlN8CxeYPEs"},"source":["### Metrics"]},{"cell_type":"markdown","metadata":{"id":"wftvJTpUYPEt"},"source":["[트레이너](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)는 트레이닝 중에 모델 성능을 자동으로 평가하지 않습니다. 메트릭을 계산하고 보고하려면 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)에 함수를 전달해야 합니다. 🤗 데이터셋 라이브러리에서는 `load_metric`(자세한 내용은 이 [튜토리얼](https://huggingface.co/docs/datasets/metrics.html) 함수를 참조하세요) 함수와 함께 로드할 수 있는 간단한 [`accuracy`](https://huggingface.co/metrics/accuracy) 함수를 제공합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3YrEnhoYPEt"},"outputs":[],"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"0T4dE7SdYPEt"},"source":["`metric`에서 `compute`를 호출하여 예측의 정확도를 계산합니다. 계산`에 예측을 전달하기 전에 예측을 로그로 변환해야 합니다(모든 🤗 트랜스포머 모델은 로그를 반환한다는 점을 기억하세요):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsu8oe3mYPEt"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{"id":"uBLY7W2fYPEt"},"source":["미세 조정 중에 평가 지표를 모니터링하려면 훈련 인수에 `evaluation_strategy` 매개 변수를 지정하여 각 에포크가 끝날 때 평가 지표를 보고하세요:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90PrvJHuYPEt"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"]},{"cell_type":"markdown","metadata":{"id":"PvIl7C95YPEt"},"source":["### Trainer"]},{"cell_type":"markdown","metadata":{"id":"veLNmLXPYPEt"},"source":["모델, 훈련 인수, 훈련 및 테스트 데이터 세트, 평가 함수를 사용하여 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 개체를 만듭니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LB34Rs1CYPEt"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_eval_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{"id":"lNkIhvl5YPEt"},"source":["Then fine-tune your model by calling [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64mBHIIwYPEt"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"co9uV5XpYPEt"},"source":["<a id='keras'></a>"]},{"cell_type":"markdown","metadata":{"id":"WSqrmVbMYPEt"},"source":["🤗 트랜스포머 모델은 Keras API를 통해 텐서플로우의 트레이닝도 지원합니다."]},{"cell_type":"markdown","metadata":{"id":"_QMc_CsmYPEt"},"source":["### Convert dataset to TensorFlow format"]},{"cell_type":"markdown","metadata":{"id":"81RTdn7VYPEt"},"source":["[DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator)는 모델이 학습할 텐서를 일괄 처리로 조립합니다. 텐서플로 텐서를 반환하려면 `return_tensors`를 지정해야 합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43IPsyWoYPEt"},"outputs":[],"source":["from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator(return_tensors=\"tf\")"]},{"cell_type":"markdown","metadata":{"id":"IYwPhTJsYPEt"},"source":["<Tip>\n","\n","[트레이너](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)는 기본적으로 [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding)을 사용하므로 데이터 콜레이터를 명시적으로 지정할 필요가 없습니다.\n","\n","</Tip>\n","\n","다음으로, [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset) 메서드를 사용하여 토큰화된 데이터셋을 텐서플로 데이터셋으로 변환합니다. 입력은 `columns`에, 레이블은 `label_cols`에 지정합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECvyhmAnYPEu"},"outputs":[],"source":["tf_train_dataset = small_train_dataset.to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")"]},{"cell_type":"markdown","metadata":{"id":"FTrHUaH3YPEu"},"source":["### Compile and fit"]},{"cell_type":"markdown","metadata":{"id":"tUYOaT9SYPEu"},"source":["예상 레이블 수로 TensorFlow 모델을 로드합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7e4ksE4YPEu"},"outputs":[],"source":["import tensorflow as tf\n","from transformers import TFAutoModelForSequenceClassification\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"]},{"cell_type":"markdown","metadata":{"id":"xWh4_YRHYPEu"},"source":["그런 다음 다른 Keras 모델과 마찬가지로 [`fit`](https://keras.io/api/models/model_training_apis/)을 사용하여 모델을 컴파일하고 미세 조정합니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZumOFlOYPEu"},"outputs":[],"source":["model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",")\n","\n","model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"0ihxfklkYPEu"},"source":["<a id='pytorch_native'></a>"]},{"cell_type":"code","source":[],"metadata":{"id":"SbGfBzrrZMrs"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/transformers_doc/training.ipynb","timestamp":1683448994633}],"machine_shape":"hm","gpuType":"T4","collapsed_sections":["_QMc_CsmYPEt","FTrHUaH3YPEu"]},"accelerator":"GPU","gpuClass":"standard","language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}