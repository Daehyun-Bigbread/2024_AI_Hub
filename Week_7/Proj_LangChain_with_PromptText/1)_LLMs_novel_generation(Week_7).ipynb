{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Fi0aF680D-DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645250fb-c8be-4881-aa48-909f0c2ad6a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Novel Generation with LLM\n",
        "* Langchain API를 이용해서 소설을 작성해볼 예정입니다.\n",
        "* 직접 프롬프트를 제작하고, 이를 Langchain api에 입력해 소설을 출력해봅니다."
      ],
      "metadata": {
        "id": "tzovwIc44Eah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain"
      ],
      "metadata": {
        "id": "t7KHOqhrEMav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbef4ce-e01a-463a-ea3e-aecc79afb517"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain)\n",
            "  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n",
            "  Downloading langchain_core-0.1.17-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.84-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.84 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.10.0 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tTIjHrG2CT54"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from pydantic import BaseModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API key\n",
        "* key 입력하기"
      ],
      "metadata": {
        "id": "CmcMh77Z5V8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "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\n",
        "\n",
        "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\n",
        "\n",
        "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\n",
        "\n",
        "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"
      ],
      "metadata": {
        "id": "z2g978HE7kET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "qxZXQzINCebv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ffe94c9-244e-489b-8f07-850c136cee1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TMP_PATH = \"/content/drive/MyDrive/dataset/Novel_generation/single_prompt/prompt_template.txt\""
      ],
      "metadata": {
        "id": "0uAt9eUFD5_B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain prompt\n",
        "* 유저 입력을 이용해서 prompt를 전달해봅시다."
      ],
      "metadata": {
        "id": "B7IySRj16lq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserRequest(BaseModel):\n",
        "    genre: str\n",
        "    characters: List[Dict[str, str]] # 캐릭터 이름, 역할 (주연, 조연)\n",
        "    idea: str\n",
        "\n",
        "\n",
        "def read_prompt_template(file_path: str) -> str:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        prompt_template = f.read()\n",
        "\n",
        "    return prompt_template\n",
        "\n",
        "\n",
        "def generate_novel(req: UserRequest) -> Dict[str, str]:\n",
        "    writer_llm = ChatOpenAI(temperature=0, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
        "    writer_prompt_template = ChatPromptTemplate.from_template(\n",
        "        template=read_prompt_template(TMP_PATH)\n",
        "    )\n",
        "    writer_chain = LLMChain(\n",
        "        llm=writer_llm, prompt=writer_prompt_template, output_key=\"output\"\n",
        "    )\n",
        "\n",
        "    result = writer_chain(req.dict())\n",
        "\n",
        "    return {\"results\": result[\"output\"]}"
      ],
      "metadata": {
        "id": "o-RVkzgKCcOo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt 참고자료 작성\n",
        "* txt 파일에 존재하는 템플릿 프롬프트를 동작시킬 프롬프트를 작성합니다.\n",
        "* user_data에 의해 작성된 데이터는 이미 생성된 템플릿 프롬프트에 추가되어 처리됩니다."
      ],
      "metadata": {
        "id": "2IOZ7FJA8gS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_data = {\n",
        "    \"genre\": \"판타지\",\n",
        "    \"characters\": [\n",
        "        {\n",
        "            \"name\": \"김철수\",\n",
        "            \"role\": \"주인공\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"이영희\",\n",
        "            \"role\": \"조연\"\n",
        "        }\n",
        "    ],\n",
        "    \"idea\": \"날씨가 추워지고 있습니다.\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "m2bfuHDHNx22"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request_instance = UserRequest(**user_data)"
      ],
      "metadata": {
        "id": "1j7O0Y3zPNu2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_novel(request_instance)"
      ],
      "metadata": {
        "id": "7WLAYEbaPP0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e398f4-1546-4cde-8ba3-5e3abeaa7136"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'results': '[소설 제목] : 얼음 마법사의 비밀\\n\\n[줄거리] :\\n김철수는 어느 날 갑작스럽게 찾아온 추운 날씨에 놀라고 있었다. 그런데 이날 밤, 김철수는 꿈 속에서 어린 시절부터 꿈꾸던 판타지 세계로 빨려들어가게 된다. 그곳에서 김철수는 얼음 마법사로부터 얼음 마법을 배울 수 있는 특별한 능력을 가지게 된다.\\n\\n김철수는 이 세계에서 이영희라는 조연과 함께 얼음 마법사의 비밀을 풀기 위해 모험을 떠나게 된다. 그들은 얼음 마법사가 어떤 이유로 인해 얼음 왕국을 위협하고 있는지 알게 되고, 그 비밀을 해결하기 위해 여러 가지 어려움과 도전을 겪게 된다.\\n\\n김철수와 이영희는 얼음 마법사의 비밀을 풀기 위해 얼음 왕국의 다양한 장소를 탐험하며, 얼음 마법을 사용하여 다양한 퍼즐과 장애물을 해결해야 한다. 그리고 그들은 얼음 왕국의 주민들과 협력하여 얼음 마법사를 막기 위한 전투에 참여하게 된다.\\n\\n이야기는 김철수와 이영희가 얼음 마법사의 비밀을 풀고 얼음 왕국을 구하는 여정을 그리며, 그들의 용감한 모험과 성장을 다룬다. 얼음 마법사의 비밀을 풀면서 김철수와 이영희는 자신의 용기와 힘을 발견하고, 진정한 영웅으로 성장하게 된다.\\n\\n[장르] : 판타지, 모험\\n\\n[세계관] : 얼음 왕국\\n\\n[키워드] : 얼음 마법, 모험, 용기, 성장, 비밀, 퍼즐, 장애물, 전투, 영웅'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}